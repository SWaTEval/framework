{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9122eda-a18a-45a4-9fda-4f57353d0e1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc6140-96bb-4136-94b7-753ff80a1c78",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10239ba0-c0ad-44b3-a005-8f5a2195d912",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_by_id(entry_id, collection):\n",
    "    for entry in collection:\n",
    "        if entry['_id']['$oid'] == entry_id:\n",
    "            return entry\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9ccb4-336a-4869-bef1-4a8ab1fbed41",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_interaction_keyword(interaction):\n",
    "    for parameter in interaction['request']['endpoint']['parameters']:\n",
    "        if parameter['name'] == 'keyword':\n",
    "            return parameter['value']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a16b1c-3f45-47c6-ad81-adfc57cdc5d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_experiment_info(root_dir, experiment_id):\n",
    "    run_directory = root_dir + '/' + experiment_id + '/mongo_data'\n",
    "    with open(run_directory + '/experiments.json') as f:\n",
    "        experiments = json.load(f)\n",
    "\n",
    "    # There should be only one entry\n",
    "    experiment_json = experiments[0]\n",
    "\n",
    "    return experiment_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c6edc-2e6e-484e-876b-a901f506ed38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_workers_info(run_dir, experiment_id):\n",
    "\n",
    "    experiment_json = get_experiment_info(run_dir, experiment_id)\n",
    "\n",
    "    columns = ['Module', 'Distance Field', 'Distance Type']\n",
    "\n",
    "    ed_module = experiment_json['workers']['endpoint_detector_module']\n",
    "    ed_module = ed_module.split('.')[2:]\n",
    "    ed_module = '.'.join(e for e in ed_module)\n",
    "    ed_field_for_distance = None\n",
    "    ed_distance_type = None\n",
    "    ed_entry = [ed_module, ed_field_for_distance, ed_distance_type]\n",
    "\n",
    "    ec_module = experiment_json['workers']['endpoint_cleaner_module']\n",
    "    ec_module = ec_module.split('.')[2:]\n",
    "    ec_module = '.'.join(e for e in ec_module)\n",
    "    ec_field_for_distance = experiment_json['endpoint_cleaner']['field_for_distance']\n",
    "    ec_distance_type = experiment_json['endpoint_cleaner']['distance_type']\n",
    "    ec_entry = [ec_module, ec_field_for_distance, ec_distance_type]\n",
    "\n",
    "    scd_module = experiment_json['workers']['state_change_detector_module']\n",
    "    scd_module = scd_module.split('.')[2:]\n",
    "    scd_module = '.'.join(e for e in scd_module)\n",
    "    scd_field_for_distance = experiment_json['state_change_detector']['field_for_distance']\n",
    "    scd_distance_type = experiment_json['state_change_detector']['distance_type']\n",
    "    scd_entry = [scd_module, scd_field_for_distance, scd_distance_type]\n",
    "\n",
    "    sc_module = experiment_json['workers']['state_collapser_module']\n",
    "    sc_module = sc_module.split('.')[2:]\n",
    "    sc_module = '.'.join(e for e in sc_module)\n",
    "    sc_field_for_distance = experiment_json['state_collapser']['field_for_distance']\n",
    "    sc_distance_type = experiment_json['state_collapser']['distance_type']\n",
    "    sc_entry = [sc_module, sc_field_for_distance, sc_distance_type]\n",
    "\n",
    "    output = pd.DataFrame()\n",
    "    output = output.append(pd.DataFrame(ed_entry).T)\n",
    "    output = output.append(pd.DataFrame(ec_entry).T)\n",
    "    output = output.append(pd.DataFrame(scd_entry).T)\n",
    "    output = output.append(pd.DataFrame(sc_entry).T)\n",
    "    output.columns = columns\n",
    "    output = output.reset_index(drop=True)\n",
    "\n",
    "    revisits = experiment_json['state_navigator']['max_revisits']\n",
    "    return output, revisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bbde21-b72c-4434-9981-a15a997881df",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_experiment_config_md5(run_dir, experiment_id):\n",
    "    full_str = ''\n",
    "    output, revisits = get_workers_info(run_dir, experiment_id)\n",
    "    for index, row in output.iterrows():\n",
    "        for col in output.columns:\n",
    "            full_str += str(row[col])\n",
    "\n",
    "    md5_hash = hashlib.md5(full_str.encode())\n",
    "    md5_hash_str = str(md5_hash.hexdigest())\n",
    "    return md5_hash_str, revisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bb933-192c-4030-bc25-713bdfdf909f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_duration_seconds(interactions):\n",
    "    if len(interactions) == 0:\n",
    "        return 0\n",
    "\n",
    "    ts_first = int(interactions[0]['created_at'])      # load timestamp as integer\n",
    "    ts_first = datetime.fromtimestamp(ts_first//1000)  # parse as datetime\n",
    "\n",
    "    ts_last = int(interactions[-1]['created_at'])\n",
    "    ts_last = datetime.fromtimestamp(ts_last//1000)\n",
    "\n",
    "    t_delta = ts_last - ts_first\n",
    "    return t_delta.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca43424-7b14-42aa-953d-426841345985",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "states_baseline_causing_interaction_keyword_pairs = list()\n",
    "states_baseline_causing_interaction_keyword_pairs.append({'previous':None, 'current':'state2_1'})\n",
    "states_baseline_causing_interaction_keyword_pairs.append({'previous':None, 'current':'state2_2'})\n",
    "states_baseline_causing_interaction_keyword_pairs.append({'previous':'state2_1', 'current':'state3_1'})\n",
    "states_baseline_causing_interaction_keyword_pairs.append({'previous':'state2_1', 'current':'state3_2'})\n",
    "states_baseline_causing_interaction_keyword_pairs.append({'previous':'state2_1', 'current':'state3_3'})\n",
    "states_baseline_causing_interaction_keyword_pairs.append({'previous':'state2_1', 'current':'state3_4'})\n",
    "states_baseline_causing_interaction_keyword_pairs.append({'previous':'state3_1', 'current':'special'})\n",
    "states_baseline_causing_interaction_keyword_pairs.append({'previous':'special', 'current':'initial'})\n",
    "state_count_baseline = len(states_baseline_causing_interaction_keyword_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54499b77-703d-4e15-802f-bdcd33f2b75c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_experiment(experiment_directory):\n",
    "\n",
    "    with open(experiment_directory + '/endpoints.json') as f:\n",
    "        endpoints = json.load(f)\n",
    "\n",
    "    with open(experiment_directory + '/interactions.json') as f:\n",
    "        interactions = json.load(f)\n",
    "\n",
    "    with open(experiment_directory + '/states.json') as f:\n",
    "        states = json.load(f)\n",
    "\n",
    "    with open(experiment_directory + '/experiments.json') as f:\n",
    "        experiments = json.load(f)\n",
    "\n",
    "    endpoint_count = len(endpoints)\n",
    "    interaction_count = len(interactions)\n",
    "    state_count = len(states)\n",
    "\n",
    "    states_causing_interaction_keyword_pairs = list()\n",
    "    for state in states:\n",
    "        \n",
    "        causing_interaction_id = state['caused_by_interaction_id']\n",
    "        causing_interaction = find_by_id(entry_id=causing_interaction_id, collection=interactions)\n",
    "\n",
    "        if causing_interaction:\n",
    "            causing_interaction_keyword = get_interaction_keyword(causing_interaction)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        previous_state_id = state['previous_state_id']\n",
    "        previous_state = find_by_id(entry_id=previous_state_id, collection=states)\n",
    "        if previous_state:\n",
    "            previous_causing_interaction_id = previous_state['caused_by_interaction_id']\n",
    "            previous_causing_interaction = find_by_id(entry_id=previous_causing_interaction_id, collection=interactions)\n",
    "            if previous_causing_interaction:\n",
    "                previous_causing_interaction_keyword = get_interaction_keyword(previous_causing_interaction)\n",
    "            else:\n",
    "                previous_causing_interaction_keyword = None\n",
    "        else:\n",
    "            previous_causing_interaction_keyword = None\n",
    "\n",
    "        keywords_pair = {'previous':previous_causing_interaction_keyword, 'current':causing_interaction_keyword}\n",
    "        states_causing_interaction_keyword_pairs.append(keywords_pair)\n",
    "\n",
    "    correctly_detected_states = 0\n",
    "    for states_causing_interaction_keyword_pair in states_causing_interaction_keyword_pairs:\n",
    "        if states_causing_interaction_keyword_pair in states_baseline_causing_interaction_keyword_pairs:\n",
    "            correctly_detected_states += 1\n",
    "\n",
    "    all_states_correct = False\n",
    "    if correctly_detected_states == 8:\n",
    "        all_states_correct = True\n",
    "\n",
    "    duration_seconds = calculate_duration_seconds(interactions)\n",
    "\n",
    "    return endpoint_count, interaction_count, state_count, correctly_detected_states, all_states_correct, duration_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61032dfa-da75-4485-a056-3549c43ba27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_for_all(root_directory, runs):\n",
    "    outputs = dict()\n",
    "    for run in tqdm(runs):\n",
    "        run_directory = f'{root_directory}/{run}'\n",
    "        run_outputs = dict()\n",
    "\n",
    "        # Load the data\n",
    "        columns=['batch_name','endpoint_count', 'interaction_count', 'state_count', 'correct_states_count', 'all_states_correct','duration_seconds', 'revisits', 'config_md5']\n",
    "        results = pd.DataFrame(columns=columns)\n",
    "\n",
    "        experiments = os.listdir(run_directory)\n",
    "        for experiment in experiments:\n",
    "            experiment_dir = f\"{run_directory}/{experiment}\"\n",
    "            if os.path.isdir(experiment_dir):\n",
    "                endpoint_count, interaction_count, state_count, correctly_detected_states, all_states_correct, duration_seconds = evaluate_experiment(f'{experiment_dir}/mongo_data')\n",
    "            config_md5, revisits = get_experiment_config_md5(run_directory, experiment)\n",
    "            df_local = pd.DataFrame([experiment, endpoint_count, interaction_count, state_count, correctly_detected_states, all_states_correct, duration_seconds, revisits, config_md5]).T\n",
    "            df_local.columns = columns\n",
    "            results = results.append(df_local, ignore_index=True)\n",
    "\n",
    "        results.duration_seconds = results.duration_seconds.astype(int)\n",
    "\n",
    "        outputs[str(run)] = results\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839633a4-e946-4c80-8acf-00d42ae677ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_data_clustering_based(data, column, title, y_lim='max', bar_offset=0):\n",
    "    state_change_clustering_selection = data['StateChangeDetector'] == 'ClusteringBased.StateChangeDetector'\n",
    "    state_change_clustering = data[state_change_clustering_selection]\n",
    "    \n",
    "    state_change_clustering_levenshtein_selection = data[state_change_clustering_selection]['StateChangeDetector_DT'] == 'levenshtein'\n",
    "    state_change_clustering_levenshtein = data[state_change_clustering_selection][state_change_clustering_levenshtein_selection]\n",
    "    \n",
    "    state_change_clustering_tlsh_selection = data[state_change_clustering_selection]['StateChangeDetector_DT'] == 'tlsh'\n",
    "    state_change_clustering_tlsh = data[state_change_clustering_selection][state_change_clustering_tlsh_selection]\n",
    "    \n",
    "    state_change_clustering_hash2vec_selection = data[state_change_clustering_selection]['StateChangeDetector_DT'] == 'hash2vec'\n",
    "    state_change_clustering_hash2vec = data[state_change_clustering_selection][state_change_clustering_hash2vec_selection]\n",
    "    \n",
    "    bar_width = 0.2\n",
    "    \n",
    "    x_values = [0]\n",
    "    position1 = [p - 1.5*bar_width for p in x_values]#[-0.3, 0.7]\n",
    "    position2 = [p - 0.5*bar_width for p in x_values]#[-0.1, 0.9]\n",
    "    position3 = [p + 0.5*bar_width for p in x_values]#[0.1, 1.1]\n",
    "    position4 = [p + 1.5*bar_width for p in x_values]#[0.3,1.3]\n",
    "    \n",
    "    #fig.xticks([r + bar_width for r in range(number_of_x_values)], ['tlsh', 'hash2vec'])\n",
    "    state_change_values = [\n",
    "                           state_change_clustering_levenshtein,\n",
    "                           state_change_clustering_tlsh,\n",
    "                           state_change_clustering_hash2vec,\n",
    "                           ]\n",
    "    state_change_labels = [\n",
    "                           \"State Change Detection: Levenshtein Distance (DOM)\",\n",
    "                           \"State Change Detection: TLSH Score (Hash)\",\n",
    "                           \"State Change Detection: Euclidean Distance (Hash)\",\n",
    "                        ]\n",
    "\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    fig.suptitle(title, fontsize=14, fontweight=\"bold\")\n",
    "    fig.subplots_adjust(hspace=0.4,wspace=0.3, top=0.85)\n",
    "    for index, state_change_selection in enumerate(state_change_values):\n",
    "        ax = fig.add_subplot(1,3,index+1)\n",
    "    \n",
    "        #ax.set_xticks(x_values, [0,1,10])\n",
    "        ax.set_title(state_change_labels[index])\n",
    "        ax.set_xlabel(\"Number of Revisits per Endpoint\")\n",
    "        ax.set_ylabel(\"Average count of correctly identified states\")\n",
    "        ax.grid(linestyle = '--', linewidth = 0.5, axis = 'y', zorder=0)\n",
    "    \n",
    "        select_hash2vec = state_change_selection['StateCollapser_DT'] == \"hash2vec\"\n",
    "        select_tlsh = state_change_selection['StateCollapser_DT'] == \"tlsh\"\n",
    "    \n",
    "        # first: State Collapser, second: Endpoint Cleaner\n",
    "        select_hash2vec_hash2vec = state_change_selection[select_hash2vec]['Endpoint_Cleaner_DT'] == \"hash2vec\"\n",
    "        select_hash2vec_tlsh = state_change_selection[select_hash2vec]['Endpoint_Cleaner_DT'] == \"tlsh\"\n",
    "        select_tlsh_hash2vec = state_change_selection[select_tlsh]['Endpoint_Cleaner_DT'] == \"hash2vec\"\n",
    "        select_tlsh_tlsh = state_change_selection[select_tlsh]['Endpoint_Cleaner_DT'] == \"tlsh\"\n",
    "        \n",
    "        hash2vec_hash2vec_plot_data = state_change_selection[select_hash2vec][select_hash2vec_hash2vec].sort_values('revisits').groupby('revisits')[column].mean()\n",
    "        hash2vec_hash2vec_plot_std = state_change_selection[select_hash2vec][select_hash2vec_hash2vec].sort_values('revisits').groupby('revisits')[column].std()\n",
    "        \n",
    "        hash2vec_tlsh_plot_data = state_change_selection[select_hash2vec][select_hash2vec_tlsh].sort_values('revisits').groupby('revisits')[column].mean()\n",
    "        hash2vec_tlsh_plot_std = state_change_selection[select_hash2vec][select_hash2vec_tlsh].sort_values('revisits').groupby('revisits')[column].std()\n",
    "        \n",
    "        tlsh_hash2vec_plot_data = state_change_selection[select_tlsh][select_tlsh_hash2vec].sort_values('revisits').groupby('revisits')[column].mean()\n",
    "        tlsh_hash2vec_plot_std = state_change_selection[select_tlsh][select_tlsh_hash2vec].sort_values('revisits').groupby('revisits')[column].std()\n",
    "        \n",
    "        tlsh_tlsh_plot_data = state_change_selection[select_tlsh][select_tlsh_tlsh].sort_values('revisits').groupby('revisits')[column].mean()\n",
    "        tlsh_tlsh_std = state_change_selection[select_tlsh][select_tlsh_tlsh].sort_values('revisits').groupby('revisits')[column].std()\n",
    "        \n",
    "        configurations_sum = hash2vec_hash2vec_plot_data + hash2vec_tlsh_plot_data + tlsh_hash2vec_plot_data + tlsh_tlsh_plot_data\n",
    "        configurations_mean = configurations_sum/4\n",
    "        \n",
    "        ax.bar(position1, hash2vec_hash2vec_plot_data, yerr=hash2vec_hash2vec_plot_std, align='center', capsize=6, zorder=3, width=bar_width, label =\"State Colapser: Euclidean Distance (Hash) and Endpoint Cleaner: Eulidean Distance (Hash)\")\n",
    "        ax.bar(position2, hash2vec_tlsh_plot_data, yerr=hash2vec_tlsh_plot_std, align='center', capsize=6, zorder=3, width=bar_width, label =\"State Colapser: Euclidean Distance (Hash) and Endpoint Cleaner: TLSH Score (Hash)\")\n",
    "        ax.bar(position3, tlsh_hash2vec_plot_data, yerr=tlsh_hash2vec_plot_std, align='center', capsize=6, zorder=3, width=bar_width, label = \"State Colapser: TLSH Score (Hash) and Endpoint Cleaner: Euclidean Distance (Hash)\")\n",
    "        ax.bar(position4, tlsh_tlsh_plot_data, yerr=tlsh_tlsh_std, align='center', capsize=6, zorder=3, width=bar_width, label =\"State Colapser: TLSH Score and Endpoint Cleaner: TLSH Score\")\n",
    "        \n",
    "        if y_lim == 'max':\n",
    "            ax.set_ylim(0,max(all_runs_df[column]) + 1.5)\n",
    "        else:\n",
    "            ax.set_ylim(0,y_lim)\n",
    "        \n",
    "        #for container in ax.containers:\n",
    "            #ax.bar_label(container)\n",
    "            \n",
    "        if index+1 == 2:\n",
    "            ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
    "              fancybox=True, shadow=True, ncol=1)\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f744dbe-5219-4e68-9dbd-0cedb5aeab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_data_one_fig_clustering_based(data, column, title, y_lim='max', y_label = \"\", bar_offset = 0):\n",
    "    bar_width = 0.2\n",
    "\n",
    "    x_values = [0,1,2]\n",
    "    position1 = [p - 1.5*bar_width for p in x_values]#[-0.3, 0.7]\n",
    "    position2 = [p - 0.5*bar_width for p in x_values]#[-0.1, 0.9]\n",
    "    position3 = [p + 0.5*bar_width for p in x_values]#[0.1, 1.1]\n",
    "    position4 = [p + 1.5*bar_width for p in x_values]#[0.3,1.3]\n",
    "    \n",
    "    euclidean_str = \"Euclidean\"\n",
    "    levenshtein_str = \"Levenshtein\"\n",
    "    tlsh_str = \"TLSH Score\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xticks(x_values, [euclidean_str, tlsh_str, levenshtein_str])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"StateChangeDetector\")\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.grid(linestyle = '--', linewidth = 0.5, axis = 'y', zorder=0)\n",
    "\n",
    "    select_hash2vec = data['StateCollapser_DT'] == \"hash2vec\"\n",
    "    select_tlsh = data['StateCollapser_DT'] == \"tlsh\"\n",
    "\n",
    "    # first: State Collapser, second: Endpoint Cleaner\n",
    "    select_hash2vec_hash2vec = data[select_hash2vec]['Endpoint_Cleaner_DT'] == \"hash2vec\"\n",
    "    select_hash2vec_tlsh = data[select_hash2vec]['Endpoint_Cleaner_DT'] == \"tlsh\"\n",
    "    select_tlsh_hash2vec = data[select_tlsh]['Endpoint_Cleaner_DT'] == \"hash2vec\"\n",
    "    select_tlsh_tlsh = data[select_tlsh]['Endpoint_Cleaner_DT'] == \"tlsh\"\n",
    "    \n",
    "\n",
    "    hash2vec_hash2vec_plot_data = data[select_hash2vec][select_hash2vec_hash2vec].sort_values('StateChangeDetector_DT').groupby('StateChangeDetector_DT')[column].mean()\n",
    "    hash2vec_hash2vec_plot_std = data[select_hash2vec][select_hash2vec_hash2vec].sort_values('StateChangeDetector_DT').groupby('StateChangeDetector_DT')[column].std()\n",
    "    hash2vec_hash2vec_plot_data = hash2vec_hash2vec_plot_data[['hash2vec', 'tlsh', 'levenshtein']]\n",
    "    hash2vec_hash2vec_plot_std = hash2vec_hash2vec_plot_std[['hash2vec', 'tlsh', 'levenshtein']]\n",
    "\n",
    "    hash2vec_tlsh_plot_data = data[select_hash2vec][select_hash2vec_tlsh].sort_values('StateChangeDetector_DT').groupby('StateChangeDetector_DT')[column].mean()\n",
    "    hash2vec_tlsh_plot_std = data[select_hash2vec][select_hash2vec_tlsh].sort_values('StateChangeDetector_DT').groupby('StateChangeDetector_DT')[column].std()\n",
    "    hash2vec_tlsh_plot_data = hash2vec_tlsh_plot_data[['hash2vec', 'tlsh', 'levenshtein']]\n",
    "    hash2vec_tlsh_plot_std = hash2vec_tlsh_plot_std[['hash2vec', 'tlsh', 'levenshtein']]\n",
    "\n",
    "    tlsh_hash2vec_plot_data = data[select_tlsh][select_tlsh_hash2vec].sort_values('StateChangeDetector_DT').groupby('StateChangeDetector_DT')[column].mean()\n",
    "    tlsh_hash2vec_plot_std = data[select_tlsh][select_tlsh_hash2vec].sort_values('StateChangeDetector_DT').groupby('StateChangeDetector_DT')[column].std()\n",
    "    tlsh_hash2vec_plot_data = tlsh_hash2vec_plot_data[['hash2vec', 'tlsh', 'levenshtein']]\n",
    "    tlsh_hash2vec_plot_std = tlsh_hash2vec_plot_std[['hash2vec', 'tlsh', 'levenshtein']]\n",
    "\n",
    "    tlsh_tlsh_plot_data = data[select_tlsh][select_tlsh_tlsh].sort_values('StateChangeDetector_DT').groupby('StateChangeDetector_DT')[column].mean()\n",
    "    tlsh_tlsh_std = data[select_tlsh][select_tlsh_tlsh].sort_values('StateChangeDetector_DT').groupby('StateChangeDetector_DT')[column].std()\n",
    "    tlsh_tlsh_plot_data = tlsh_tlsh_plot_data[['hash2vec', 'tlsh', 'levenshtein']]\n",
    "    tlsh_tlsh_std = tlsh_tlsh_std[['hash2vec', 'tlsh', 'levenshtein']]\n",
    "\n",
    "    ax.bar(position1, hash2vec_hash2vec_plot_data, yerr=hash2vec_hash2vec_plot_std, align='center', capsize=6, zorder=3, width=bar_width, label =f\"StateDetector: {euclidean_str} | EndpointDetector: {euclidean_str}\")\n",
    "    ax.bar(position2, hash2vec_tlsh_plot_data, yerr=hash2vec_tlsh_plot_std, align='center', capsize=6, zorder=3, width=bar_width, label =f\"StateDetector: {euclidean_str} | EndpointDetector: {tlsh_str}\")\n",
    "    ax.bar(position3, tlsh_hash2vec_plot_data, yerr=tlsh_hash2vec_plot_std, align='center', capsize=6, zorder=3, width=bar_width, label = f\"StateDetector: {tlsh_str} | EndpointDetector: {euclidean_str}\")\n",
    "    ax.bar(position4, tlsh_tlsh_plot_data, yerr=tlsh_tlsh_std, align='center', capsize=6, zorder=3, width=bar_width, label =f\"StateDetector: {tlsh_str} | EndpointDetector: {tlsh_str}\")\n",
    "\n",
    "    if y_lim == 'max':\n",
    "        ax.set_ylim(0,max(all_runs_df[column]) + 1.5)\n",
    "    else:\n",
    "        ax.set_ylim(0,y_lim)\n",
    "        \n",
    "    print(hash2vec_hash2vec_plot_std)\n",
    "    print(hash2vec_tlsh_plot_std)\n",
    "    print(tlsh_hash2vec_plot_std)\n",
    "    print(tlsh_tlsh_std)\n",
    "\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
    "      fancybox=True, shadow=False, ncol=1)\n",
    "    plt.axhline(8, color='black', linestyle=\"--\")\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.svg', bbox_inches='tight', format=\"svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a3b94-62d7-4e81-a629-0067aff74862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_data_basic(data, column, title, y_lim='max', bar_offset=0):\n",
    "    state_change_clustering_selection = data['StateChangeDetector'] == 'ClusteringBased.StateChangeDetector'\n",
    "    state_change_clustering = data[state_change_clustering_selection]\n",
    "    \n",
    "    state_change_clustering_levenshtein_selection = data[state_change_clustering_selection]['StateChangeDetector_DT'] == 'levenshtein'\n",
    "    state_change_clustering_levenshtein = data[state_change_clustering_selection][state_change_clustering_levenshtein_selection]\n",
    "    \n",
    "    state_change_clustering_tlsh_selection = data[state_change_clustering_selection]['StateChangeDetector_DT'] == 'tlsh'\n",
    "    state_change_clustering_tlsh = data[state_change_clustering_selection][state_change_clustering_tlsh_selection]\n",
    "    \n",
    "    state_change_clustering_hash2vec_selection = data[state_change_clustering_selection]['StateChangeDetector_DT'] == 'hash2vec'\n",
    "    state_change_clustering_hash2vec = data[state_change_clustering_selection][state_change_clustering_hash2vec_selection]\n",
    "    \n",
    "    bar_width = 0.2\n",
    "    \n",
    "    x_values = [0]\n",
    "    position1 = [p - 1.5*bar_width for p in x_values]#[-0.3, 0.7]\n",
    "    position2 = [p - 0.5*bar_width for p in x_values]#[-0.1, 0.9]\n",
    "    position3 = [p + 0.5*bar_width for p in x_values]#[0.1, 1.1]\n",
    "    position4 = [p + 1.5*bar_width for p in x_values]#[0.3,1.3]\n",
    "    \n",
    "    #fig.xticks([r + bar_width for r in range(number_of_x_values)], ['tlsh', 'hash2vec'])\n",
    "    state_change_values = [\n",
    "                           state_change_clustering_levenshtein,\n",
    "                           state_change_clustering_tlsh,\n",
    "                           state_change_clustering_hash2vec,\n",
    "                           ]\n",
    "    state_change_labels = [\n",
    "                           \"State Change Detection: Levenshtein Distance (DOM)\",\n",
    "                           \"State Change Detection: TLSH Score (Hash)\",\n",
    "                           \"State Change Detection: Euclidean Distance (Hash)\",\n",
    "                        ]\n",
    "\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    fig.suptitle(title, fontsize=14, fontweight=\"bold\")\n",
    "    fig.subplots_adjust(hspace=0.4,wspace=0.3, top=0.85)\n",
    "    for index, state_change_selection in enumerate(state_change_values):\n",
    "        ax = fig.add_subplot(1,3,index+1)\n",
    "    \n",
    "        #ax.set_xticks(x_values, [0,1,10])\n",
    "        ax.set_title(state_change_labels[index])\n",
    "        ax.set_xlabel(\"Number of Revisits per Endpoint\")\n",
    "        ax.set_ylabel(\"Average count of correctly identified states\")\n",
    "        ax.grid(linestyle = '--', linewidth = 0.5, axis = 'y', zorder=0)\n",
    "    \n",
    "        select_hash2vec = state_change_selection['StateCollapser_DT'] == \"hash2vec\"\n",
    "        select_tlsh = state_change_selection['StateCollapser_DT'] == \"tlsh\"\n",
    "        \n",
    "        hash2vec_plot_data = state_change_selection[select_hash2vec].sort_values('revisits').groupby('revisits')[column].mean()\n",
    "        hash2vec_std = state_change_selection[select_hash2vec].sort_values('revisits').groupby('revisits')[column].std()\n",
    "        \n",
    "        tlsh_plot_data = state_change_selection[select_tlsh].sort_values('revisits').groupby('revisits')[column].mean()\n",
    "        tlsh_std = state_change_selection[select_tlsh].sort_values('revisits').groupby('revisits')[column].std()\n",
    "        \n",
    "        configurations_sum = hash2vec_plot_data + tlsh_plot_data\n",
    "        configurations_mean = configurations_sum/2\n",
    "        \n",
    "        ax.bar(position1, hash2vec_plot_data, yerr=hash2vec_std, align='center', capsize=6, zorder=3, width=bar_width, label =\"State Colapser: Euclidean Distance (Hash)\")\n",
    "        ax.bar(position2, tlsh_plot_data, yerr=tlsh_std, align='center', capsize=6, zorder=3, width=bar_width, label =\"State Colapser: TLSH Score (Hash)\")\n",
    "        #ax.bar(position3, tlsh_hash2vec_plot_data, yerr=tlsh_hash2vec_plot_std, align='center', capsize=6, zorder=3, width=bar_width, label = \"State Colapser: TLSH Score (Hash) and Endpoint Cleaner: Euclidean Distance (Hash)\")\n",
    "        #ax.bar(position4, tlsh_tlsh_plot_data, yerr=tlsh_tlsh_std, align='center', capsize=6, zorder=3, width=bar_width, label =\"State Colapser: TLSH Score and Endpoint Cleaner: TLSH Score\")\n",
    "        \n",
    "        if y_lim == 'max':\n",
    "            ax.set_ylim(0,max(all_runs_df[column]) + 1.5)\n",
    "        else:\n",
    "            ax.set_ylim(0,y_lim)\n",
    "        \n",
    "        #for container in ax.containers:\n",
    "            #ax.bar_label(container)\n",
    "            \n",
    "        if index+1 == 2:\n",
    "            ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
    "              fancybox=True, shadow=True, ncol=1)\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65e021-0275-4ead-a0fd-7f97579c4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_data_one_fig_basic(data, column, title, y_lim='max', y_label = \"\", bar_offset = 0):\n",
    "    bar_width = 0.2\n",
    "\n",
    "    x_values = [0,0.5,1]\n",
    "    position1 = [p - 1.5*bar_width for p in x_values]#[-0.3, 0.7]\n",
    "    position2 = [p - 0.5*bar_width for p in x_values]#[-0.1, 0.9]\n",
    "    position3 = [p + 0.5*bar_width for p in x_values]#[0.1, 1.1]\n",
    "    position4 = [p + 1.5*bar_width for p in x_values]#[0.3,1.3]\n",
    "\n",
    "    euclidean_str = \"Euclidean\"\n",
    "    levenshtein_str = \"Levenshtein\"\n",
    "    tlsh_str = \"TLSH Score\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (6,3))\n",
    "    ax.set_xticks(x_values, [euclidean_str, tlsh_str, levenshtein_str])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"StateChangeDetector\")\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.grid(linestyle = '--', linewidth = 0.5, axis = 'y', zorder=0)\n",
    "\n",
    "    select_hash2vec = data['StateCollapser_DT'] == \"hash2vec\"\n",
    "    select_tlsh = data['StateCollapser_DT'] == \"tlsh\"\n",
    "    \n",
    "    hash2vec_plot_data = data[select_hash2vec].sort_values('StateChangeDetector_DT').groupby('StateChangeDetector_DT')[column].mean()\n",
    "    hash2vec_plot_std = data[select_hash2vec].sort_values('StateChangeDetector_DT').groupby('StateChangeDetector_DT')[column].std()\n",
    "    hash2vec_plot_data = hash2vec_plot_data[['hash2vec', 'tlsh', 'levenshtein']]\n",
    "    hash2vec_plot_std = hash2vec_plot_std[['hash2vec', 'tlsh', 'levenshtein']]\n",
    "    \n",
    "    tlsh_plot_data = data[select_tlsh].sort_values('StateChangeDetector_DT').groupby('StateChangeDetector_DT')[column].mean()\n",
    "    tlsh_plot_std = data[select_tlsh].sort_values('StateChangeDetector_DT').groupby('StateChangeDetector_DT')[column].std()\n",
    "    tlsh_plot_data = tlsh_plot_data[['hash2vec', 'tlsh', 'levenshtein']]\n",
    "    tlsh_plot_std = tlsh_plot_std[['hash2vec', 'tlsh', 'levenshtein']]\n",
    "\n",
    "    ax.bar(position2, hash2vec_plot_data, yerr=hash2vec_plot_std, align='center', capsize=6, zorder=3, width=bar_width, label =f\"StateDetector: {euclidean_str}\")\n",
    "    ax.bar(position3, tlsh_plot_data, yerr=tlsh_plot_std, align='center', capsize=6, zorder=3, width=bar_width, label =f\"StateDetector: {tlsh_str}\")\n",
    "    #ax.bar(position3, tlsh_hash2vec_plot_data, yerr=tlsh_hash2vec_plot_std, align='center', capsize=6, zorder=3, width=bar_width, label = f\"StateDetector: {tlsh_str} | EndpointDetector: {euclidean_str}\")\n",
    "    #ax.bar(position4, tlsh_tlsh_plot_data, yerr=tlsh_tlsh_std, align='center', capsize=6, zorder=3, width=bar_width, label =f\"StateDetector: {tlsh_str} | EndpointDetector: {tlsh_str}\")\n",
    "\n",
    "    if y_lim == 'max':\n",
    "        ax.set_ylim(0,max(all_runs_df[column]) + 1.5)\n",
    "    else:\n",
    "        ax.set_ylim(0,y_lim)\n",
    "\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
    "      fancybox=True, shadow=False, ncol=2)\n",
    "    plt.axhline(8, color='black', linestyle=\"--\")\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}_no_ec.svg', bbox_inches='tight', format=\"svg\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385387a6-cab4-4ce6-af8e-e2f87f435880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_data(data, column, module_column, module_name):\n",
    "    clustering_module_selection = data[module_column] == module_name\n",
    "    \n",
    "    clustering_module_levenshtein_selection = data[clustering_module_selection][f'{module_column}_DT'] == 'levenshtein'\n",
    "    clustering_module_levenshtein = data[clustering_module_selection][clustering_module_levenshtein_selection]\n",
    "    \n",
    "    clustering_module_tlsh_selection = data[clustering_module_selection][f'{module_column}_DT'] == 'tlsh'\n",
    "    clustering_module_tlsh = data[clustering_module_selection][clustering_module_tlsh_selection]\n",
    "    \n",
    "    clustering_module_hash2vec_selection = data[clustering_module_selection][f'{module_column}_DT'] == 'hash2vec'\n",
    "    clustering_module_hash2vec = data[clustering_module_selection][clustering_module_hash2vec_selection]\n",
    "    \n",
    "    \n",
    "    module_values = [clustering_module_levenshtein,\n",
    "                     clustering_module_tlsh,\n",
    "                     clustering_module_hash2vec]\n",
    "    \n",
    "    output = []\n",
    "        \n",
    "    for index, module_selection in enumerate(module_values):\n",
    "        output.append(module_selection[column].mean())\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b44844-6304-44d6-b8c0-850e43e7fd12",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define root folder of evaluation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c401ea4-9e55-489e-9c62-0fa81f0b3857",
   "metadata": {},
   "source": [
    "**Note:** The root_directory variable should point to the `evaluation-data` repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca246b1f-2047-4bba-ac30-d917554090e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"**path to the evaluation data**\"\n",
    "run_count = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376f5c0-ca8b-41c9-8451-ef7a082c2b4f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Clustering Based Endpoint Detector Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6318c-fdad-4d31-b265-9fcb62ddbd61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments_dir = root_directory + '/Clustering Based ED Experiments'\n",
    "all_runs_eval = gen_for_all(experiments_dir, range(run_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832fa49",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Tabular Representation of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb40dc9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating one big picture\n",
    "\n",
    "# Data generation\n",
    "\n",
    "columns = [\n",
    "    \"EndpointDetector\", \"EndpointDetector_DF\", \"EndpointDetector_DT\",\n",
    "    \"Endpoint_Cleaner\", \"Endpoint_Cleaner_DF\", \"Endpoint_Cleaner_DT\",\n",
    "    \"StateChangeDetector\", \"StateChangeDetector_DF\", \"StateChangeDetector_DT\",\n",
    "    \"StateCollapser\", \"StateCollapser_DF\", \"StateCollapser_DT\",\n",
    "]\n",
    "\n",
    "all_runs_list = []\n",
    "columns_combined = all_runs_eval['0'].columns.values.tolist() + columns\n",
    "all_runs_df = pd.DataFrame(columns=columns_combined)\n",
    "for run, eval_data in all_runs_eval.items():\n",
    "    all_results_reshaped = pd.DataFrame(columns = columns_combined)\n",
    "    for index, experiment in eval_data.iterrows():\n",
    "        run_directory = f\"{experiments_dir}/{run}\"\n",
    "        config, revisits = get_workers_info(run_directory, str(experiment.batch_name))\n",
    "        config_reshaped = pandas.DataFrame(columns=columns_combined)\n",
    "        tmp_dict = experiment.to_dict()\n",
    "        for i, row in config.iterrows():\n",
    "            tmp_dict[columns[i*3]] = row['Module']\n",
    "            tmp_dict[columns[i*3+1]] = row['Distance Field']\n",
    "            tmp_dict[columns[i*3+2]] = row['Distance Type']\n",
    "        config_reshaped = config_reshaped.append(tmp_dict, ignore_index=True)\n",
    "        all_results_reshaped = all_results_reshaped.append(config_reshaped, ignore_index=True)\n",
    "    all_runs_list.append(all_results_reshaped)\n",
    "    all_runs_df = all_runs_df.append(all_results_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36063850-508a-49f5-9c30-64a991abfeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_data_clustering_based(data=all_runs_df, \n",
    "               column='correct_states_count', \n",
    "               y_lim=10,\n",
    "               bar_offset=0.04,\n",
    "               title='Average count of correctly identified states in all possible configuration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154e2be-a619-4d5c-897c-e3165129df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_data_clustering_based(data=all_runs_df, \n",
    "               column='state_count', \n",
    "               y_lim=10,\n",
    "               bar_offset=0.04,\n",
    "               title='Average detected state count in all possible configuration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a94a9-21fb-4d72-81f6-16684ed0525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_data_clustering_based(data=all_runs_df, \n",
    "               column='interaction_count',\n",
    "               y_lim=400,\n",
    "               bar_offset=0.04,\n",
    "               title='Average interaction count in all possible configuration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0559629a-3e90-4a76-bea8-ca7bfc7f31fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_data_clustering_based(data=all_runs_df, \n",
    "               column='endpoint_count',\n",
    "               y_lim=1500,\n",
    "               bar_offset=0.06,\n",
    "               title='Average endpoint count in all possible configuration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c20d6-9813-483f-aa3d-976e63a2f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_data_one_fig_clustering_based(data=all_runs_df,\n",
    "               column='correct_states_count',\n",
    "               y_lim=9,\n",
    "               bar_offset=0.04,\n",
    "               title='Average Number of Correctly Identified States',\n",
    "               y_label='Number of correctly identified States') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4bec6-ad38-4014-9e36-069fae2c9daa",
   "metadata": {},
   "source": [
    "## Calculate the impact of each module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05503ec6-d73a-404b-8da0-4bd8c18b2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values_scd = calculate_mean_data(all_runs_df, 'correct_states_count','StateChangeDetector' ,'ClusteringBased.StateChangeDetector')\n",
    "diff_scd = max(mean_values_scd)-min(mean_values_scd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984545c7-4bf4-4b14-9b37-a0403d470681",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values_sc = calculate_mean_data(all_runs_df, 'correct_states_count','StateCollapser' ,'ClusteringBased.StateCollapser')\n",
    "mean_values_sc.pop(0)\n",
    "diff_sc = max(mean_values_sc)-min(mean_values_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761c6a9-45bb-4189-8c09-45c223d40843",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values_ec = calculate_mean_data(all_runs_df, 'correct_states_count','Endpoint_Cleaner' ,'ClusteringBased.EndpointCleaner')\n",
    "mean_values_ec.pop(0)\n",
    "diff_ec = max(mean_values_ec)-min(mean_values_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf48a5a-29a8-483a-a8cd-28746465cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "title = \"Maximum Difference of Correctly Identified States\"\n",
    "ax.set_title(title)\n",
    "ax.set_xlabel(\"Detector\")\n",
    "ax.set_ylabel(\"Maximum Difference of Correctly Identified States\")\n",
    "x_values = [0,1,2]\n",
    "ax.grid(linestyle = '--', linewidth = 0.5, axis = 'y', zorder=0)\n",
    "ax.set_xticks(x_values, [\"StateChangeDetector\", \"StateDetector\", \"EndpointDetector\"])\n",
    "ax.set_yticks([0,1,2,3])\n",
    "bars = ax.bar(x_values,(diff_scd, diff_sc, diff_ec), zorder=3)\n",
    "ax.bar_label(bars)\n",
    "plt.savefig(f'{title.lower().replace(\" \", \"_\")}.svg', bbox_inches='tight', format=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5425c4ff-fe83-43a6-a1c5-2cdb59a3ec3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basic Endpoint Detector Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa95d4e-0a6c-40ee-ad52-bf10df49f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_dir = root_directory + '/Basic ED Experiments'\n",
    "all_runs_eval = gen_for_all(experiments_dir, range(run_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6773895-19a5-49d2-90a6-39ff6f025669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating one big picture\n",
    "\n",
    "# Data generation\n",
    "\n",
    "columns = [\n",
    "    \"EndpointDetector\", \"EndpointDetector_DF\", \"EndpointDetector_DT\",\n",
    "    \"Endpoint_Cleaner\", \"Endpoint_Cleaner_DF\", \"Endpoint_Cleaner_DT\",\n",
    "    \"StateChangeDetector\", \"StateChangeDetector_DF\", \"StateChangeDetector_DT\",\n",
    "    \"StateCollapser\", \"StateCollapser_DF\", \"StateCollapser_DT\",\n",
    "]\n",
    "\n",
    "all_runs_list = []\n",
    "columns_combined = all_runs_eval['0'].columns.values.tolist() + columns\n",
    "all_runs_df = pd.DataFrame(columns=columns_combined)\n",
    "for run, eval_data in all_runs_eval.items():\n",
    "    all_results_reshaped = pd.DataFrame(columns = columns_combined)\n",
    "    for index, experiment in eval_data.iterrows():\n",
    "        run_directory = f\"{experiments_dir}/{run}\"\n",
    "        config, revisits = get_workers_info(run_directory, str(experiment.batch_name))\n",
    "        config_reshaped = pandas.DataFrame(columns=columns_combined)\n",
    "        tmp_dict = experiment.to_dict()\n",
    "        for i, row in config.iterrows():\n",
    "            tmp_dict[columns[i*3]] = row['Module']\n",
    "            tmp_dict[columns[i*3+1]] = row['Distance Field']\n",
    "            tmp_dict[columns[i*3+2]] = row['Distance Type']\n",
    "        config_reshaped = config_reshaped.append(tmp_dict, ignore_index=True)\n",
    "        all_results_reshaped = all_results_reshaped.append(config_reshaped, ignore_index=True)\n",
    "    all_runs_list.append(all_results_reshaped)\n",
    "    all_runs_df = all_runs_df.append(all_results_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b993cb-52ae-4f9f-8b66-cce498648c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_data_basic(data=all_runs_df, \n",
    "               column='correct_states_count', \n",
    "               y_lim=9,\n",
    "               bar_offset=0.04,\n",
    "               title='Average count of correctly identified states in all possible configuration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45285110-00c6-4b3a-9e6e-37b24f3a1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_data_basic(data=all_runs_df, \n",
    "               column='state_count', \n",
    "               y_lim=10,\n",
    "               bar_offset=0.04,\n",
    "               title='Average detected state count in all possible configuration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6739e-344e-4de5-b6d3-5c2b518c3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_data_basic(data=all_runs_df, \n",
    "               column='interaction_count',\n",
    "               y_lim=350,\n",
    "               bar_offset=0.04,\n",
    "               title='Average interaction count in all possible configuration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a148b9a-449f-4f09-99ed-677a20071049",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_data_basic(data=all_runs_df, \n",
    "               column='endpoint_count',\n",
    "               y_lim=1500,\n",
    "               bar_offset=0.06,\n",
    "               title='Average endpoint count in all possible configuration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7523f4-15dc-451a-974b-a8c1d2f0608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_data_one_fig_basic(data=all_runs_df,\n",
    "               column='correct_states_count',\n",
    "               y_lim=9,\n",
    "               bar_offset=0.04,\n",
    "               title='Average Number of Correctly Identified States',\n",
    "               y_label='Number of correctly identified States') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
